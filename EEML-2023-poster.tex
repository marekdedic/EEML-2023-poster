\documentclass{beamer}

\usepackage{polyglossia}

\usepackage[orientation = portrait, size = a0, scale = 1.4]{beamerposter}
\usepackage[backend = biber, style = iso-authoryear, sortlocale = en_US, autolang = other, bibencoding = UTF8]{biblatex}
\usepackage{datetime}
\usepackage[utf8]{inputenc}
\usepackage{fontspec}
\usepackage{microtype}

\setdefaultlanguage{english}
\setmainfont{TeX Gyre Termes}
\usetheme{gemini}
\usecolortheme{mit}

\addbibresource{zotero.bib}
\AtBeginBibliography{\small}

\title{Balancing performance and complexity with adaptive graph coarsening}

\newdate{presentation}{11}{07}{2022}
\date{EEML 2023, \displaydate{presentation}}

\author{Marek Dědič\inst{1, 2}}

\institute{
	\inst{1} Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University in Prague \samelineand
	\inst{2} Cognitive Intelligence, Cisco Systems, Inc.
}


\footercontent{
	EEML 2023, Košice \hfill
	\href{mailto:marek@dedic.eu}{marek@dedic.eu}
}

% \logoright{\includegraphics[height=7cm]{logo1.pdf}}
% \logoleft{\includegraphics[height=7cm]{logo2.pdf}}

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.033\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

\begin{document}

\begin{frame}[t]

\begin{columns}[t]
	\separatorcolumn

	\begin{column}{\colwidth}
		\begin{block}{Motivation}
			Graph neural networks (GNNs) have proven to achieve superior performance on a number of graph datasets and are adopted in industrial applications across many fields. Superior GNN performance is, however, often paid for by a numerically intensive training procedure with a significant memory footprint, which may be a prohibitive issue in some applications. We investigate the interplay of graph coarsening and the quality of its learned embedding, which in turn entails an interplay between the coarsening and the performance of a downstream task, in our case, node classification.
			\begin{figure}
				\includegraphics[width=0.6\linewidth]{images/coarsening-illustration/coarsening-illustration.pdf}
			\end{figure}
		\end{block}

		\begin{block}{The performance-complexity trade-off}
			The main aim of this work is to explore the performance-complexity characteristics in the context of graph learning, as introduced in \cite{prochazka_downstream_2022}. The result of a repeated application of graph coarsening is a sequence of graphs \( G_0, G_1, G_2, \dots, G_L \) where \( G_0 = G \). Given a model \( M \) that operates on graphs, a performance metric, and a complexity metric, the sequence \( G_0, G_1, \dots, G_L \) corresponds to points in the performance-complexity plane, where advancing along the sequence generally hurts performance and decreases complexity. This performance-complexity characteristic allows for a choice of a working point that is optimal for the particular use-case.

			\begin{figure}
				\includegraphics[width=0.7\linewidth]{images/performance-complexity/performance-complexity.pdf}
			\end{figure}
		\end{block}

		\begin{block}{HARP pipeline overview}
			\begin{figure}
				\includegraphics[width=\linewidth]{images/harp-overview/harp-overview.pdf}
			\end{figure}
		\end{block}
	\end{column}

	\separatorcolumn

	\begin{column}{\colwidth}
		\begin{block}{Deep HARP}
			\begin{figure}
				\includegraphics[width=0.5\linewidth]{images/deep-harp/deep-harp.pdf}
			\end{figure}
		\end{block}

		\begin{block}{Adaptive prolongation}
			\begin{figure}
				\includegraphics[width=\linewidth]{images/adaptive-prolongation/adaptive-prolongation.pdf}
			\end{figure}
		\end{block}

		\begin{block}{Results}
			\begin{figure}
				\includegraphics[width = \linewidth]{images/adaptive-coarsening/adaptive-coarsening.pdf}
			\end{figure}
			Downstream classifier accuracies at different steps of adaptive prolongation. Dashed line shows the baseline node2vec model accuracy. The node count is taken relative to the total node count in each dataset. The results are averaged over multiple runs, with the solid line representing the mean and the shaded area denoting one standard deviation.
		\end{block}

		\begin{block}{References}
			\printbibliography
		\end{block}
	\end{column}

	\separatorcolumn
\end{columns}

\end{frame}

\end{document}
